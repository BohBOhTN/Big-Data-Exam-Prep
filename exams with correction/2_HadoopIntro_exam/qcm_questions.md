# QCM - Introduction à Hadoop

## Chapitre 2 : Introduction au Framework Hadoop

**Auteur du cours:** Mohamed KOUBAA  
**Nombre de questions:** 40

---

### Question 1
Hadoop est un framework écrit dans quel langage de programmation ?

- A) Python
- B) C++
- C) Java
- D) Scala

---

### Question 2
Quels sont les 3 modules principaux du framework Hadoop de base ?

- A) HDFS, Hive, Pig
- B) HDFS, YARN, MapReduce
- C) MapReduce, Spark, HBase
- D) YARN, Zookeeper, Flume

---

### Question 3
Que signifie l'acronyme HDFS ?

- A) Hadoop Data File Storage
- B) High Distributed File System
- C) Hadoop Distributed File System
- D) Hybrid Data File System

---

### Question 4
Que signifie l'acronyme YARN ?

- A) Yet Another Resource Navigator
- B) Yet Another Resource Negotiator
- C) Yarn Application Resource Node
- D) Yield And Resource Network

---

### Question 5
Quel est le principe fondamental du traitement dans Hadoop ?

- A) Copier les données vers un serveur central pour le traitement
- B) Traiter les données là où elles sont stockées
- C) Utiliser uniquement la mémoire vive pour le traitement
- D) Compresser les données avant le traitement

---

### Question 6
Quelle est la caractéristique qui permet d'ajouter des machines au cluster selon les besoins ?

- A) Haute disponibilité
- B) Tolérance aux pannes
- C) Scalabilité
- D) Sécurité

---

### Question 7
Quel outil de l'écosystème Hadoop permet d'écrire des scripts avec le langage Pig Latin ?

- A) Hive
- B) Pig
- C) Sqoop
- D) Flume

---

### Question 8
Quel outil utilise un langage proche de SQL (HiveQL) pour interroger les données ?

- A) Pig
- B) Impala
- C) Hive
- D) HBase

---

### Question 9
Quel outil est utilisé pour l'ordonnancement des jobs MapReduce et la définition de workflows ?

- A) Zookeeper
- B) Oozie
- C) Ambari
- D) Flume

---

### Question 10
HBase est une base de données NoSQL de quel type ?

- A) Orientée documents
- B) Orientée colonnes
- C) Orientée graphes
- D) Clé-valeur simple

---

### Question 11
Quel outil permet de collecter des logs et de les stocker dans HDFS ?

- A) Sqoop
- B) Flume
- C) Kafka
- D) Storm

---

### Question 12
Quel outil permet la lecture et l'écriture des données à partir de bases de données externes relationnelles ?

- A) Flume
- B) Kafka
- C) Sqoop
- D) HBase

---

### Question 13
Quel outil est utilisé pour le provisionnement, la gestion et le monitoring des clusters Hadoop ?

- A) Zookeeper
- B) Oozie
- C) Ambari
- D) Mahout

---

### Question 14
Zookeeper est un service centralisé pour :

- A) Le stockage des données
- B) Le traitement MapReduce
- C) La maintenance des informations de configuration et la synchronisation distribuée
- D) La collecte de logs

---

### Question 15
Mahout est une bibliothèque utilisée pour :

- A) La visualisation de données
- B) Le Machine Learning et les mathématiques
- C) La gestion des workflows
- D) Le stockage NoSQL

---

### Question 16
Impala permet de requêter les données directement depuis :

- A) MongoDB uniquement
- B) HDFS et HBase avec HiveSQL
- C) Des bases de données relationnelles
- D) Apache Kafka

---

### Question 17
Quelle capacité de stockage HDFS peut-il gérer ?

- A) Gigaoctets
- B) Téraoctets
- C) Pétaoctets
- D) Mégaoctets

---

### Question 18
Combien de machines un cluster Hadoop peut-il potentiellement gérer ?

- A) Dizaines de nœuds
- B) Centaines de nœuds
- C) Milliers de nœuds
- D) Une seule machine

---

### Question 19
Hadoop est un projet :

- A) Propriétaire Microsoft
- B) Propriétaire Google
- C) Open source
- D) Propriétaire Amazon

---

### Question 20
Qu'est-ce qu'un cluster Hadoop ?

- A) Un seul serveur puissant
- B) Une collection de machines sur lesquelles les données sont sauvegardées et traitées
- C) Un logiciel de visualisation
- D) Une base de données relationnelle

---

### Question 21
Laquelle des caractéristiques suivantes N'EST PAS une caractéristique de Hadoop ?

- A) Haute disponibilité
- B) Traitement temps réel uniquement
- C) Tolérance aux pannes
- D) Scalabilité

---

### Question 22
Les connecteurs R dans l'écosystème Hadoop permettent :

- A) Le stockage des données uniquement
- B) L'accès à HDFS et l'exécution de requêtes Map/Reduce via le langage R
- C) La gestion des clusters
- D) Le monitoring des applications

---

### Question 23
Quel est le rôle principal de YARN dans Hadoop ?

- A) Stockage des données
- B) Gestion des ressources du cluster
- C) Traitement des requêtes SQL
- D) Collecte des logs

---

### Question 24
Quel est le rôle principal de MapReduce dans Hadoop ?

- A) Stockage distribué des données
- B) Gestion des ressources
- C) Traitement distribué des données
- D) Monitoring du cluster

---

### Question 25
Parmi les outils suivants, lequel fonctionne directement au-dessus de HDFS (pas via YARN/MapReduce) ?

- A) Pig
- B) Hive
- C) HBase
- D) Mahout

---

### Question 26
L'écosystème Hadoop permet toutes ces fonctions SAUF :

- A) L'extraction et le stockage des données
- B) La simplification des opérations de traitement
- C) La gestion et coordination de la plateforme
- D) Le remplacement complet des bases de données relationnelles pour tous les cas d'usage

---

### Question 27
Quelle caractéristique permet à Hadoop de continuer à fonctionner même en cas de défaillance matérielle ?

- A) Scalabilité
- B) Tolérance aux pannes
- C) HPC
- D) Sécurité

---

### Question 28
HPC dans le contexte Hadoop signifie :

- A) Hadoop Processing Center
- B) High Performance Computing
- C) Hadoop Protocol Configuration
- D) High Priority Cluster

---

### Question 29
Quel outil de l'écosystème Hadoop est développé par Cloudera pour les requêtes SQL ?

- A) Hive
- B) Phoenix
- C) Impala
- D) Pig

---

### Question 30
Les données dans Hadoop sont d'abord divisées puis :

- A) Compressées et envoyées à un serveur central
- B) Sauvegardées sur un cluster et traitées localement
- C) Converties en format propriétaire
- D) Archivées et supprimées

---

### Question 31
Quel est l'avantage principal de traiter les données là où elles sont stockées ?

- A) Meilleure compression
- B) Réduction du coût de transfert de données
- C) Meilleure sécurité
- D) Format de données uniforme

---

### Question 32
Combien de catégories d'outils composent l'écosystème Hadoop selon le cours ?

- A) 2
- B) 3
- C) 4
- D) 5

---

### Question 33
Apache Phoenix est un moteur de base de données relationnelle construit sur :

- A) MongoDB
- B) Cassandra
- C) HBase
- D) HDFS directement

---

### Question 34
Quel outil permet de définir des workflows de jobs dans Hadoop ?

- A) Ambari
- B) Zookeeper
- C) Oozie
- D) Flume

---

### Question 35
La scalabilité horizontale dans Hadoop signifie :

- A) Ajouter plus de RAM à un serveur existant
- B) Ajouter plus de disques à un serveur existant
- C) Ajouter plus de machines au cluster
- D) Augmenter la vitesse du processeur

---

### Question 36
Quel composant permet la reprise après échec dans Hadoop ?

- A) Les mécanismes de tolérance aux pannes intégrés
- B) Les sauvegardes manuelles uniquement
- C) Le redémarrage manuel du cluster
- D) Un serveur de secours externe

---

### Question 37
Le monitoring du cluster Hadoop peut être fait avec :

- A) Pig
- B) Hive
- C) Ambari
- D) Sqoop

---

### Question 38
Flume est principalement utilisé pour :

- A) Les requêtes SQL
- B) La collecte de logs
- C) Le Machine Learning
- D) La gestion des workflows

---

### Question 39
Quelle affirmation est correcte concernant Hadoop ?

- A) Il nécessite du matériel très coûteux
- B) Il peut fonctionner sur du matériel standard (commodity hardware)
- C) Il ne supporte que les données structurées
- D) Il est limité à un seul nœud

---

### Question 40
Parmi les outils suivants, lequel N'EST PAS un outil de connexion aux sources externes ?

- A) Sqoop
- B) Flume
- C) Mahout
- D) Kafka (dans certaines configurations)

---

## Fin du QCM

**Instructions:** Pour chaque question, sélectionnez la meilleure réponse parmi les options proposées (A, B, C ou D).
