# QCM - Introduction Ã  Hadoop
## Questions avec Corrections et Conseils

**Auteur du cours:** Mohamed KOUBAA  
**Nombre de questions:** 40

---

### Question 1
**Hadoop est un framework Ã©crit dans quel langage de programmation ?**

- A) Python
- B) C++
- **C) Java âœ“**
- D) Scala

**âœ… RÃ©ponse: C**

ğŸ’¡ **Conseil:** Hadoop est un framework **libre et open source Ã©crit en Java**. Cela permet une grande portabilitÃ© et une large communautÃ© de dÃ©veloppeurs.

---

### Question 2
**Quels sont les 3 modules principaux du framework Hadoop de base ?**

- A) HDFS, Hive, Pig
- **B) HDFS, YARN, MapReduce âœ“**
- C) MapReduce, Spark, HBase
- D) YARN, Zookeeper, Flume

**âœ… RÃ©ponse: B**

ğŸ’¡ **Conseil:** Le cÅ“ur de Hadoop = **HDFS** (stockage) + **YARN** (gestion des ressources) + **MapReduce** (traitement). Hive, Pig, etc. font partie de l'Ã©cosystÃ¨me mais pas du framework de base.

---

### Question 3
**Que signifie l'acronyme HDFS ?**

- A) Hadoop Data File Storage
- B) High Distributed File System
- **C) Hadoop Distributed File System âœ“**
- D) Hybrid Data File System

**âœ… RÃ©ponse: C**

ğŸ’¡ **Conseil:** **H**adoop **D**istributed **F**ile **S**ystem - le systÃ¨me de fichiers distribuÃ©s qui permet de stocker des pÃ©taoctets de donnÃ©es sur un cluster.

---

### Question 4
**Que signifie l'acronyme YARN ?**

- A) Yet Another Resource Navigator
- **B) Yet Another Resource Negotiator âœ“**
- C) Yarn Application Resource Node
- D) Yield And Resource Network

**âœ… RÃ©ponse: B**

ğŸ’¡ **Conseil:** **Y**et **A**nother **R**esource **N**egotiator - YARN gÃ¨re l'allocation des ressources (CPU, mÃ©moire) entre les applications du cluster.

---

### Question 5
**Quel est le principe fondamental du traitement dans Hadoop ?**

- A) Copier les donnÃ©es vers un serveur central pour le traitement
- **B) Traiter les donnÃ©es lÃ  oÃ¹ elles sont stockÃ©es âœ“**
- C) Utiliser uniquement la mÃ©moire vive pour le traitement
- D) Compresser les donnÃ©es avant le traitement

**âœ… RÃ©ponse: B**

ğŸ’¡ **Conseil:** C'est le principe clÃ© de Hadoop : **dÃ©placer le calcul vers les donnÃ©es, pas l'inverse**. Cela Ã©vite les coÃ»ts de transfert de pÃ©taoctets de donnÃ©es sur le rÃ©seau.

---

### Question 6
**Quelle est la caractÃ©ristique qui permet d'ajouter des machines au cluster selon les besoins ?**

- A) Haute disponibilitÃ©
- B) TolÃ©rance aux pannes
- **C) ScalabilitÃ© âœ“**
- D) SÃ©curitÃ©

**âœ… RÃ©ponse: C**

ğŸ’¡ **Conseil:** **ScalabilitÃ© horizontale** = ajouter des machines. Hadoop peut scaler de quelques nÅ“uds Ã  des milliers de nÅ“uds selon les besoins en stockage et en calcul.

---

### Question 7
**Quel outil de l'Ã©cosystÃ¨me Hadoop permet d'Ã©crire des scripts avec le langage Pig Latin ?**

- A) Hive
- **B) Pig âœ“**
- C) Sqoop
- D) Flume

**âœ… RÃ©ponse: B**

ğŸ’¡ **Conseil:** **Pig** â†’ **Pig Latin** (langage procÃ©dural de script). **Hive** â†’ **HiveQL** (SQL-like dÃ©claratif). Les deux simplifient l'Ã©criture de jobs MapReduce.

---

### Question 8
**Quel outil utilise un langage proche de SQL (HiveQL) pour interroger les donnÃ©es ?**

- A) Pig
- B) Impala
- **C) Hive âœ“**
- D) HBase

**âœ… RÃ©ponse: C**

ğŸ’¡ **Conseil:** **Hive** = Data Warehouse avec HiveQL. **Impala** utilise aussi HiveQL mais pour des requÃªtes directes sur HDFS/HBase sans MapReduce.

---

### Question 9
**Quel outil est utilisÃ© pour l'ordonnancement des jobs MapReduce et la dÃ©finition de workflows ?**

- A) Zookeeper
- **B) Oozie âœ“**
- C) Ambari
- D) Flume

**âœ… RÃ©ponse: B**

ğŸ’¡ **Conseil:** **Oozie** = orchestrateur/scheduler de workflows Hadoop. Il permet de chaÃ®ner des jobs MapReduce, Pig, Hive en sÃ©quence ou en parallÃ¨le avec gestion des dÃ©pendances.

---

### Question 10
**HBase est une base de donnÃ©es NoSQL de quel type ?**

- A) OrientÃ©e documents
- **B) OrientÃ©e colonnes âœ“**
- C) OrientÃ©e graphes
- D) ClÃ©-valeur simple

**âœ… RÃ©ponse: B**

ğŸ’¡ **Conseil:** **HBase** = NoSQL orientÃ© colonnes (column-family store), inspirÃ© de Google BigTable. **MongoDB** = orientÃ© documents. **Neo4j** = orientÃ© graphes.

---

### Question 11
**Quel outil permet de collecter des logs et de les stocker dans HDFS ?**

- A) Sqoop
- **B) Flume âœ“**
- C) Kafka
- D) Storm

**âœ… RÃ©ponse: B**

ğŸ’¡ **Conseil:** **Flume** = collecte de logs/Ã©vÃ©nements vers HDFS. **Sqoop** = transfert depuis/vers SGBD relationnels. Pensez "Flume = Flux de logs".

---

### Question 12
**Quel outil permet la lecture et l'Ã©criture des donnÃ©es Ã  partir de bases de donnÃ©es externes relationnelles ?**

- A) Flume
- B) Kafka
- **C) Sqoop âœ“**
- D) HBase

**âœ… RÃ©ponse: C**

ğŸ’¡ **Conseil:** **Sqoop** = "SQL to Hadoop" - outil ETL bidirectionnel entre SGBD relationnels (MySQL, Oracle, etc.) et Hadoop (HDFS, Hive, HBase).

---

### Question 13
**Quel outil est utilisÃ© pour le provisionnement, la gestion et le monitoring des clusters Hadoop ?**

- A) Zookeeper
- B) Oozie
- **C) Ambari âœ“**
- D) Mahout

**âœ… RÃ©ponse: C**

ğŸ’¡ **Conseil:** **Ambari** = interface web pour administrer et monitorer les clusters Hadoop. **Zookeeper** = coordination des services distribuÃ©s (pas de GUI).

---

### Question 14
**Zookeeper est un service centralisÃ© pour :**

- A) Le stockage des donnÃ©es
- B) Le traitement MapReduce
- **C) La maintenance des informations de configuration et la synchronisation distribuÃ©e âœ“**
- D) La collecte de logs

**âœ… RÃ©ponse: C**

ğŸ’¡ **Conseil:** **Zookeeper** = "gardien du zoo" qui coordonne les services distribuÃ©s : configuration, nommage, synchronisation, Ã©lection de leader. Essentiel pour Kafka, HBase, etc.

---

### Question 15
**Mahout est une bibliothÃ¨que utilisÃ©e pour :**

- A) La visualisation de donnÃ©es
- **B) Le Machine Learning et les mathÃ©matiques âœ“**
- C) La gestion des workflows
- D) Le stockage NoSQL

**âœ… RÃ©ponse: B**

ğŸ’¡ **Conseil:** **Mahout** = bibliothÃ¨que ML pour Hadoop (clustering, classification, recommandations). Aujourd'hui souvent remplacÃ© par Spark MLlib.

---

### Question 16
**Impala permet de requÃªter les donnÃ©es directement depuis :**

- A) MongoDB uniquement
- **B) HDFS et HBase avec HiveSQL âœ“**
- C) Des bases de donnÃ©es relationnelles
- D) Apache Kafka

**âœ… RÃ©ponse: B**

ğŸ’¡ **Conseil:** **Impala** (Cloudera) = moteur SQL massively parallel qui interroge HDFS/HBase directement, sans passer par MapReduce. Plus rapide pour les requÃªtes interactives.

---

### Question 17
**Quelle capacitÃ© de stockage HDFS peut-il gÃ©rer ?**

- A) Gigaoctets
- B) TÃ©raoctets
- **C) PÃ©taoctets âœ“**
- D) MÃ©gaoctets

**âœ… RÃ©ponse: C**

ğŸ’¡ **Conseil:** HDFS est conÃ§u pour des **pÃ©taoctets de donnÃ©es** (1 Po = 1000 To). Les grands clusters (Yahoo, Facebook) stockent des centaines de pÃ©taoctets.

---

### Question 18
**Combien de machines un cluster Hadoop peut-il potentiellement gÃ©rer ?**

- A) Dizaines de nÅ“uds
- B) Centaines de nÅ“uds
- **C) Milliers de nÅ“uds âœ“**
- D) Une seule machine

**âœ… RÃ©ponse: C**

ğŸ’¡ **Conseil:** Hadoop peut scaler jusqu'Ã  des **milliers de nÅ“uds**. Les plus grands clusters dÃ©passent 10,000 nÅ“uds. C'est la scalabilitÃ© horizontale.

---

### Question 19
**Hadoop est un projet :**

- A) PropriÃ©taire Microsoft
- B) PropriÃ©taire Google
- **C) Open source âœ“**
- D) PropriÃ©taire Amazon

**âœ… RÃ©ponse: C**

ğŸ’¡ **Conseil:** Hadoop est un projet **Apache Software Foundation**, libre et open source. Il est inspirÃ© des papiers de Google (GFS, MapReduce) mais n'est pas un produit Google.

---

### Question 20
**Qu'est-ce qu'un cluster Hadoop ?**

- A) Un seul serveur puissant
- **B) Une collection de machines sur lesquelles les donnÃ©es sont sauvegardÃ©es et traitÃ©es âœ“**
- C) Un logiciel de visualisation
- D) Une base de donnÃ©es relationnelle

**âœ… RÃ©ponse: B**

ğŸ’¡ **Conseil:** Un **cluster** = ensemble de machines (nÅ“uds) travaillant ensemble. Dans Hadoop, les donnÃ©es sont distribuÃ©es ET le traitement est parallÃ©lisÃ© sur ces machines.

---

### Question 21
**Laquelle des caractÃ©ristiques suivantes N'EST PAS une caractÃ©ristique de Hadoop ?**

- A) Haute disponibilitÃ©
- **B) Traitement temps rÃ©el uniquement âœ“**
- C) TolÃ©rance aux pannes
- D) ScalabilitÃ©

**âœ… RÃ©ponse: B**

ğŸ’¡ **Conseil:** Hadoop (MapReduce) est conÃ§u pour le **traitement par lots (batch)**, pas le temps rÃ©el. Pour le temps rÃ©el, on utilise Storm, Flink, Spark Streaming.

---

### Question 22
**Les connecteurs R dans l'Ã©cosystÃ¨me Hadoop permettent :**

- A) Le stockage des donnÃ©es uniquement
- **B) L'accÃ¨s Ã  HDFS et l'exÃ©cution de requÃªtes Map/Reduce via le langage R âœ“**
- C) La gestion des clusters
- D) Le monitoring des applications

**âœ… RÃ©ponse: B**

ğŸ’¡ **Conseil:** Les **R Connectors** permettent aux data scientists d'utiliser R pour analyser des donnÃ©es Hadoop sans apprendre Java ou Pig Latin.

---

### Question 23
**Quel est le rÃ´le principal de YARN dans Hadoop ?**

- A) Stockage des donnÃ©es
- **B) Gestion des ressources du cluster âœ“**
- C) Traitement des requÃªtes SQL
- D) Collecte des logs

**âœ… RÃ©ponse: B**

ğŸ’¡ **Conseil:** **YARN** = Resource Manager global du cluster. Il alloue CPU et mÃ©moire aux applications (MapReduce ou autres) de maniÃ¨re dynamique.

---

### Question 24
**Quel est le rÃ´le principal de MapReduce dans Hadoop ?**

- A) Stockage distribuÃ© des donnÃ©es
- B) Gestion des ressources
- **C) Traitement distribuÃ© des donnÃ©es âœ“**
- D) Monitoring du cluster

**âœ… RÃ©ponse: C**

ğŸ’¡ **Conseil:** **MapReduce** = modÃ¨le de programmation pour le traitement parallÃ¨le. Map (transformation) + Reduce (agrÃ©gation) sur les donnÃ©es distribuÃ©es.

---

### Question 25
**Parmi les outils suivants, lequel fonctionne directement au-dessus de HDFS (pas via YARN/MapReduce) ?**

- A) Pig
- B) Hive
- **C) HBase âœ“**
- D) Mahout

**âœ… RÃ©ponse: C**

ğŸ’¡ **Conseil:** **HBase** et **Impala** fonctionnent directement sur HDFS. Pig, Hive, Mahout gÃ©nÃ¨rent des jobs MapReduce qui passent par YARN.

---

### Question 26
**L'Ã©cosystÃ¨me Hadoop permet toutes ces fonctions SAUF :**

- A) L'extraction et le stockage des donnÃ©es
- B) La simplification des opÃ©rations de traitement
- C) La gestion et coordination de la plateforme
- **D) Le remplacement complet des bases de donnÃ©es relationnelles pour tous les cas d'usage âœ“**

**âœ… RÃ©ponse: D**

ğŸ’¡ **Conseil:** Hadoop complÃ¨te mais ne remplace pas totalement les SGBD relationnels. Les SGBD sont toujours meilleurs pour les transactions ACID et les requÃªtes Ã  faible latence.

---

### Question 27
**Quelle caractÃ©ristique permet Ã  Hadoop de continuer Ã  fonctionner mÃªme en cas de dÃ©faillance matÃ©rielle ?**

- A) ScalabilitÃ©
- **B) TolÃ©rance aux pannes âœ“**
- C) HPC
- D) SÃ©curitÃ©

**âœ… RÃ©ponse: B**

ğŸ’¡ **Conseil:** **TolÃ©rance aux pannes** = rÃ©plication des donnÃ©es (facteur 3 par dÃ©faut) + redÃ©marrage automatique des tÃ¢ches en cas de panne d'un nÅ“ud.

---

### Question 28
**HPC dans le contexte Hadoop signifie :**

- A) Hadoop Processing Center
- **B) High Performance Computing âœ“**
- C) Hadoop Protocol Configuration
- D) High Priority Cluster

**âœ… RÃ©ponse: B**

ğŸ’¡ **Conseil:** **HPC** = High Performance Computing. Hadoop permet le calcul haute performance en parallÃ©lisant le traitement sur des milliers de nÅ“uds.

---

### Question 29
**Quel outil de l'Ã©cosystÃ¨me Hadoop est dÃ©veloppÃ© par Cloudera pour les requÃªtes SQL ?**

- A) Hive
- B) Phoenix
- **C) Impala âœ“**
- D) Pig

**âœ… RÃ©ponse: C**

ğŸ’¡ **Conseil:** **Impala** = projet Cloudera pour requÃªtes SQL interactives sur HDFS/HBase. **Phoenix** = projet Apache pour SQL sur HBase.

---

### Question 30
**Les donnÃ©es dans Hadoop sont d'abord divisÃ©es puis :**

- A) CompressÃ©es et envoyÃ©es Ã  un serveur central
- **B) SauvegardÃ©es sur un cluster et traitÃ©es localement âœ“**
- C) Converties en format propriÃ©taire
- D) ArchivÃ©es et supprimÃ©es

**âœ… RÃ©ponse: B**

ğŸ’¡ **Conseil:** Principe Hadoop : **Diviser â†’ Distribuer â†’ Traiter localement â†’ AgrÃ©ger**. Pas de centralisation des donnÃ©es pour le traitement.

---

### Question 31
**Quel est l'avantage principal de traiter les donnÃ©es lÃ  oÃ¹ elles sont stockÃ©es ?**

- A) Meilleure compression
- **B) RÃ©duction du coÃ»t de transfert de donnÃ©es âœ“**
- C) Meilleure sÃ©curitÃ©
- D) Format de donnÃ©es uniforme

**âœ… RÃ©ponse: B**

ğŸ’¡ **Conseil:** TransfÃ©rer 20 To sur le rÃ©seau = heures/jours. Envoyer du code (quelques Ko) vers les donnÃ©es = secondes. C'est l'essence de "data locality".

---

### Question 32
**Combien de catÃ©gories d'outils composent l'Ã©cosystÃ¨me Hadoop selon le cours ?**

- A) 2
- B) 3
- **C) 4 âœ“**
- D) 5

**âœ… RÃ©ponse: C**

ğŸ’¡ **Conseil:** 4 catÃ©gories : 1) Outils au-dessus de YARN/MR (Pig, Hive...) 2) Outils sur HDFS (HBase, Impala) 3) Connexion externe (Sqoop, Flume) 4) Administration (Ambari, Zookeeper).

---

### Question 33
**Apache Phoenix est un moteur de base de donnÃ©es relationnelle construit sur :**

- A) MongoDB
- B) Cassandra
- **C) HBase âœ“**
- D) HDFS directement

**âœ… RÃ©ponse: C**

ğŸ’¡ **Conseil:** **Phoenix** ajoute une couche SQL sur HBase, permettant d'utiliser des requÃªtes SQL standards sur une base NoSQL orientÃ©e colonnes.

---

### Question 34
**Quel outil permet de dÃ©finir des workflows de jobs dans Hadoop ?**

- A) Ambari
- B) Zookeeper
- **C) Oozie âœ“**
- D) Flume

**âœ… RÃ©ponse: C**

ğŸ’¡ **Conseil:** **Oozie** = scheduler de workflows. Il permet de dÃ©finir des DAG (graphes orientÃ©s acycliques) de jobs avec dÃ©pendances et conditions.

---

### Question 35
**La scalabilitÃ© horizontale dans Hadoop signifie :**

- A) Ajouter plus de RAM Ã  un serveur existant
- B) Ajouter plus de disques Ã  un serveur existant
- **C) Ajouter plus de machines au cluster âœ“**
- D) Augmenter la vitesse du processeur

**âœ… RÃ©ponse: C**

ğŸ’¡ **Conseil:** **Horizontale** = ajouter des machines (scale out). **Verticale** = amÃ©liorer une machine existante (scale up). Hadoop privilÃ©gie l'horizontal.

---

### Question 36
**Quel composant permet la reprise aprÃ¨s Ã©chec dans Hadoop ?**

- **A) Les mÃ©canismes de tolÃ©rance aux pannes intÃ©grÃ©s âœ“**
- B) Les sauvegardes manuelles uniquement
- C) Le redÃ©marrage manuel du cluster
- D) Un serveur de secours externe

**âœ… RÃ©ponse: A**

ğŸ’¡ **Conseil:** Hadoop gÃ¨re automatiquement les pannes : rÃ©plication des blocs HDFS, redÃ©marrage des tÃ¢ches MapReduce Ã©chouÃ©es, basculement NameNode (HA).

---

### Question 37
**Le monitoring du cluster Hadoop peut Ãªtre fait avec :**

- A) Pig
- B) Hive
- **C) Ambari âœ“**
- D) Sqoop

**âœ… RÃ©ponse: C**

ğŸ’¡ **Conseil:** **Ambari** = interface web pour l'administration et le monitoring. Il affiche les mÃ©triques, alertes, et permet de gÃ©rer les services du cluster.

---

### Question 38
**Flume est principalement utilisÃ© pour :**

- A) Les requÃªtes SQL
- **B) La collecte de logs âœ“**
- C) Le Machine Learning
- D) La gestion des workflows

**âœ… RÃ©ponse: B**

ğŸ’¡ **Conseil:** **Flume** = collecteur de donnÃ©es en streaming (logs, Ã©vÃ©nements) vers HDFS ou HBase. Sources : fichiers logs, syslog, HTTP, etc.

---

### Question 39
**Quelle affirmation est correcte concernant Hadoop ?**

- A) Il nÃ©cessite du matÃ©riel trÃ¨s coÃ»teux
- **B) Il peut fonctionner sur du matÃ©riel standard (commodity hardware) âœ“**
- C) Il ne supporte que les donnÃ©es structurÃ©es
- D) Il est limitÃ© Ã  un seul nÅ“ud

**âœ… RÃ©ponse: B**

ğŸ’¡ **Conseil:** Hadoop est conÃ§u pour fonctionner sur du **matÃ©riel standard** (commodity hardware), pas des serveurs haut de gamme. Cela rÃ©duit considÃ©rablement les coÃ»ts.

---

### Question 40
**Parmi les outils suivants, lequel N'EST PAS un outil de connexion aux sources externes ?**

- A) Sqoop
- B) Flume
- **C) Mahout âœ“**
- D) Kafka (dans certaines configurations)

**âœ… RÃ©ponse: C**

ğŸ’¡ **Conseil:** **Mahout** = bibliothÃ¨que de Machine Learning, pas un connecteur de donnÃ©es. Sqoop (SGBD), Flume (logs), Kafka (streaming) sont des outils d'ingestion de donnÃ©es.

---

## ğŸ“Š RÃ©sumÃ© des ThÃ¨mes AbordÃ©s

| ThÃ¨me | Nombre de Questions |
|-------|---------------------|
| Architecture Hadoop (HDFS, YARN, MR) | 10 |
| Ã‰cosystÃ¨me et outils | 15 |
| CaractÃ©ristiques (scalabilitÃ©, tolÃ©rance...) | 8 |
| Concepts fondamentaux | 7 |

---

## ğŸ¯ Conseils pour l'Examen

1. **MÃ©morisez les 3 composants de base** : HDFS + YARN + MapReduce

2. **Connaissez le rÃ´le de chaque outil** :
   - Stockage : HDFS, HBase
   - Traitement : MapReduce, Pig, Hive
   - Ingestion : Sqoop (SGBD), Flume (logs)
   - Administration : Ambari, Zookeeper, Oozie

3. **Comprenez le principe clÃ©** : "DÃ©placer le calcul vers les donnÃ©es"

4. **Retenez les caractÃ©ristiques** : ScalabilitÃ©, TolÃ©rance aux pannes, Haute disponibilitÃ©

5. **Distinguez les outils** :
   - Pig = Pig Latin (procÃ©dural)
   - Hive = HiveQL (SQL-like)
   - Impala = SQL direct sur HDFS (Cloudera)

---

**Bonne chance pour votre examen ! ğŸ€**
