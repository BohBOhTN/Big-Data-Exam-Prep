{
  "topic_name": "Introduction √† Hadoop",
  "topic_description": "Questions avec Corrections et Conseils",
  "author": "Mohamed KOUBAA",
  "num_questions": 40,
  "questions": [
    {
      "question_number": 1,
      "question_text": "Hadoop est un framework √©crit dans quel langage de programmation ?",
      "options": [
        {
          "letter": "A",
          "text": "Python",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "C++",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "Java",
          "is_correct": true
        },
        {
          "letter": "D",
          "text": "Scala",
          "is_correct": false
        }
      ],
      "correct_answer": "C",
      "conseil": "Hadoop est un framework libre et open source √©crit en Java. Cela permet une grande portabilit√© et une large communaut√© de d√©veloppeurs."
    },
    {
      "question_number": 2,
      "question_text": "Quels sont les 3 modules principaux du framework Hadoop de base ?",
      "options": [
        {
          "letter": "A",
          "text": "HDFS, Hive, Pig",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "HDFS, YARN, MapReduce",
          "is_correct": true
        },
        {
          "letter": "C",
          "text": "MapReduce, Spark, HBase",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "YARN, Zookeeper, Flume",
          "is_correct": false
        }
      ],
      "correct_answer": "B",
      "conseil": "Le c≈ìur de Hadoop = HDFS (stockage) + YARN (gestion des ressources) + MapReduce (traitement). Hive, Pig, etc. font partie de l'√©cosyst√®me mais pas du framework de base."
    },
    {
      "question_number": 3,
      "question_text": "Que signifie l'acronyme HDFS ?",
      "options": [
        {
          "letter": "A",
          "text": "Hadoop Data File Storage",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "High Distributed File System",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "Hadoop Distributed File System",
          "is_correct": true
        },
        {
          "letter": "D",
          "text": "Hybrid Data File System",
          "is_correct": false
        }
      ],
      "correct_answer": "C",
      "conseil": "Hadoop Distributed File System - le syst√®me de fichiers distribu√©s qui permet de stocker des p√©taoctets de donn√©es sur un cluster."
    },
    {
      "question_number": 4,
      "question_text": "Que signifie l'acronyme YARN ?",
      "options": [
        {
          "letter": "A",
          "text": "Yet Another Resource Navigator",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Yet Another Resource Negotiator",
          "is_correct": true
        },
        {
          "letter": "C",
          "text": "Yarn Application Resource Node",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "Yield And Resource Network",
          "is_correct": false
        }
      ],
      "correct_answer": "B",
      "conseil": "Yet Another Resource Negotiator - YARN g√®re l'allocation des ressources (CPU, m√©moire) entre les applications du cluster."
    },
    {
      "question_number": 5,
      "question_text": "Quel est le principe fondamental du traitement dans Hadoop ?",
      "options": [
        {
          "letter": "A",
          "text": "Copier les donn√©es vers un serveur central pour le traitement",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Traiter les donn√©es l√† o√π elles sont stock√©es",
          "is_correct": true
        },
        {
          "letter": "C",
          "text": "Utiliser uniquement la m√©moire vive pour le traitement",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "Compresser les donn√©es avant le traitement",
          "is_correct": false
        }
      ],
      "correct_answer": "B",
      "conseil": "C'est le principe cl√© de Hadoop : d√©placer le calcul vers les donn√©es, pas l'inverse. Cela √©vite les co√ªts de transfert de p√©taoctets de donn√©es sur le r√©seau."
    },
    {
      "question_number": 6,
      "question_text": "Quelle est la caract√©ristique qui permet d'ajouter des machines au cluster selon les besoins ?",
      "options": [
        {
          "letter": "A",
          "text": "Haute disponibilit√©",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Tol√©rance aux pannes",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "Scalabilit√©",
          "is_correct": true
        },
        {
          "letter": "D",
          "text": "S√©curit√©",
          "is_correct": false
        }
      ],
      "correct_answer": "C",
      "conseil": "Scalabilit√© horizontale = ajouter des machines. Hadoop peut scaler de quelques n≈ìuds √† des milliers de n≈ìuds selon les besoins en stockage et en calcul."
    },
    {
      "question_number": 7,
      "question_text": "Quel outil de l'√©cosyst√®me Hadoop permet d'√©crire des scripts avec le langage Pig Latin ?",
      "options": [
        {
          "letter": "A",
          "text": "Hive",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Pig",
          "is_correct": true
        },
        {
          "letter": "C",
          "text": "Sqoop",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "Flume",
          "is_correct": false
        }
      ],
      "correct_answer": "B",
      "conseil": "Pig ‚Üí Pig Latin (langage proc√©dural de script). Hive ‚Üí HiveQL (SQL-like d√©claratif). Les deux simplifient l'√©criture de jobs MapReduce."
    },
    {
      "question_number": 8,
      "question_text": "Quel outil utilise un langage proche de SQL (HiveQL) pour interroger les donn√©es ?",
      "options": [
        {
          "letter": "A",
          "text": "Pig",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Impala",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "Hive",
          "is_correct": true
        },
        {
          "letter": "D",
          "text": "HBase",
          "is_correct": false
        }
      ],
      "correct_answer": "C",
      "conseil": "Hive = Data Warehouse avec HiveQL. Impala utilise aussi HiveQL mais pour des requ√™tes directes sur HDFS/HBase sans MapReduce."
    },
    {
      "question_number": 9,
      "question_text": "Quel outil est utilis√© pour l'ordonnancement des jobs MapReduce et la d√©finition de workflows ?",
      "options": [
        {
          "letter": "A",
          "text": "Zookeeper",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Oozie",
          "is_correct": true
        },
        {
          "letter": "C",
          "text": "Ambari",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "Flume",
          "is_correct": false
        }
      ],
      "correct_answer": "B",
      "conseil": "Oozie = orchestrateur/scheduler de workflows Hadoop. Il permet de cha√Æner des jobs MapReduce, Pig, Hive en s√©quence ou en parall√®le avec gestion des d√©pendances."
    },
    {
      "question_number": 10,
      "question_text": "HBase est une base de donn√©es NoSQL de quel type ?",
      "options": [
        {
          "letter": "A",
          "text": "Orient√©e documents",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Orient√©e colonnes",
          "is_correct": true
        },
        {
          "letter": "C",
          "text": "Orient√©e graphes",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "Cl√©-valeur simple",
          "is_correct": false
        }
      ],
      "correct_answer": "B",
      "conseil": "HBase = NoSQL orient√© colonnes (column-family store), inspir√© de Google BigTable. MongoDB = orient√© documents. Neo4j = orient√© graphes."
    },
    {
      "question_number": 11,
      "question_text": "Quel outil permet de collecter des logs et de les stocker dans HDFS ?",
      "options": [
        {
          "letter": "A",
          "text": "Sqoop",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Flume",
          "is_correct": true
        },
        {
          "letter": "C",
          "text": "Kafka",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "Storm",
          "is_correct": false
        }
      ],
      "correct_answer": "B",
      "conseil": "Flume = collecte de logs/√©v√©nements vers HDFS. Sqoop = transfert depuis/vers SGBD relationnels. Pensez \"Flume = Flux de logs\"."
    },
    {
      "question_number": 12,
      "question_text": "Quel outil permet la lecture et l'√©criture des donn√©es √† partir de bases de donn√©es externes relationnelles ?",
      "options": [
        {
          "letter": "A",
          "text": "Flume",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Kafka",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "Sqoop",
          "is_correct": true
        },
        {
          "letter": "D",
          "text": "HBase",
          "is_correct": false
        }
      ],
      "correct_answer": "C",
      "conseil": "Sqoop = \"SQL to Hadoop\" - outil ETL bidirectionnel entre SGBD relationnels (MySQL, Oracle, etc.) et Hadoop (HDFS, Hive, HBase)."
    },
    {
      "question_number": 13,
      "question_text": "Quel outil est utilis√© pour le provisionnement, la gestion et le monitoring des clusters Hadoop ?",
      "options": [
        {
          "letter": "A",
          "text": "Zookeeper",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Oozie",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "Ambari",
          "is_correct": true
        },
        {
          "letter": "D",
          "text": "Mahout",
          "is_correct": false
        }
      ],
      "correct_answer": "C",
      "conseil": "Ambari = interface web pour administrer et monitorer les clusters Hadoop. Zookeeper = coordination des services distribu√©s (pas de GUI)."
    },
    {
      "question_number": 14,
      "question_text": "Zookeeper est un service centralis√© pour :",
      "options": [
        {
          "letter": "A",
          "text": "Le stockage des donn√©es",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Le traitement MapReduce",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "La maintenance des informations de configuration et la synchronisation distribu√©e",
          "is_correct": true
        },
        {
          "letter": "D",
          "text": "La collecte de logs",
          "is_correct": false
        }
      ],
      "correct_answer": "C",
      "conseil": "Zookeeper = \"gardien du zoo\" qui coordonne les services distribu√©s : configuration, nommage, synchronisation, √©lection de leader. Essentiel pour Kafka, HBase, etc."
    },
    {
      "question_number": 15,
      "question_text": "Mahout est une biblioth√®que utilis√©e pour :",
      "options": [
        {
          "letter": "A",
          "text": "La visualisation de donn√©es",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Le Machine Learning et les math√©matiques",
          "is_correct": true
        },
        {
          "letter": "C",
          "text": "La gestion des workflows",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "Le stockage NoSQL",
          "is_correct": false
        }
      ],
      "correct_answer": "B",
      "conseil": "Mahout = biblioth√®que ML pour Hadoop (clustering, classification, recommandations). Aujourd'hui souvent remplac√© par Spark MLlib."
    },
    {
      "question_number": 16,
      "question_text": "Impala permet de requ√™ter les donn√©es directement depuis :",
      "options": [
        {
          "letter": "A",
          "text": "MongoDB uniquement",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "HDFS et HBase avec HiveSQL",
          "is_correct": true
        },
        {
          "letter": "C",
          "text": "Des bases de donn√©es relationnelles",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "Apache Kafka",
          "is_correct": false
        }
      ],
      "correct_answer": "B",
      "conseil": "Impala (Cloudera) = moteur SQL massively parallel qui interroge HDFS/HBase directement, sans passer par MapReduce. Plus rapide pour les requ√™tes interactives."
    },
    {
      "question_number": 17,
      "question_text": "Quelle capacit√© de stockage HDFS peut-il g√©rer ?",
      "options": [
        {
          "letter": "A",
          "text": "Gigaoctets",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "T√©raoctets",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "P√©taoctets",
          "is_correct": true
        },
        {
          "letter": "D",
          "text": "M√©gaoctets",
          "is_correct": false
        }
      ],
      "correct_answer": "C",
      "conseil": "HDFS est con√ßu pour des p√©taoctets de donn√©es (1 Po = 1000 To). Les grands clusters (Yahoo, Facebook) stockent des centaines de p√©taoctets."
    },
    {
      "question_number": 18,
      "question_text": "Combien de machines un cluster Hadoop peut-il potentiellement g√©rer ?",
      "options": [
        {
          "letter": "A",
          "text": "Dizaines de n≈ìuds",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Centaines de n≈ìuds",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "Milliers de n≈ìuds",
          "is_correct": true
        },
        {
          "letter": "D",
          "text": "Une seule machine",
          "is_correct": false
        }
      ],
      "correct_answer": "C",
      "conseil": "Hadoop peut scaler jusqu'√† des milliers de n≈ìuds. Les plus grands clusters d√©passent 10,000 n≈ìuds. C'est la scalabilit√© horizontale."
    },
    {
      "question_number": 19,
      "question_text": "Hadoop est un projet :",
      "options": [
        {
          "letter": "A",
          "text": "Propri√©taire Microsoft",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Propri√©taire Google",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "Open source",
          "is_correct": true
        },
        {
          "letter": "D",
          "text": "Propri√©taire Amazon",
          "is_correct": false
        }
      ],
      "correct_answer": "C",
      "conseil": "Hadoop est un projet Apache Software Foundation, libre et open source. Il est inspir√© des papiers de Google (GFS, MapReduce) mais n'est pas un produit Google."
    },
    {
      "question_number": 20,
      "question_text": "Qu'est-ce qu'un cluster Hadoop ?",
      "options": [
        {
          "letter": "A",
          "text": "Un seul serveur puissant",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Une collection de machines sur lesquelles les donn√©es sont sauvegard√©es et trait√©es",
          "is_correct": true
        },
        {
          "letter": "C",
          "text": "Un logiciel de visualisation",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "Une base de donn√©es relationnelle",
          "is_correct": false
        }
      ],
      "correct_answer": "B",
      "conseil": "Un cluster = ensemble de machines (n≈ìuds) travaillant ensemble. Dans Hadoop, les donn√©es sont distribu√©es ET le traitement est parall√©lis√© sur ces machines."
    },
    {
      "question_number": 21,
      "question_text": "Laquelle des caract√©ristiques suivantes N'EST PAS une caract√©ristique de Hadoop ?",
      "options": [
        {
          "letter": "A",
          "text": "Haute disponibilit√©",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Traitement temps r√©el uniquement",
          "is_correct": true
        },
        {
          "letter": "C",
          "text": "Tol√©rance aux pannes",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "Scalabilit√©",
          "is_correct": false
        }
      ],
      "correct_answer": "B",
      "conseil": "Hadoop (MapReduce) est con√ßu pour le traitement par lots (batch), pas le temps r√©el. Pour le temps r√©el, on utilise Storm, Flink, Spark Streaming."
    },
    {
      "question_number": 22,
      "question_text": "Les connecteurs R dans l'√©cosyst√®me Hadoop permettent :",
      "options": [
        {
          "letter": "A",
          "text": "Le stockage des donn√©es uniquement",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "L'acc√®s √† HDFS et l'ex√©cution de requ√™tes Map/Reduce via le langage R",
          "is_correct": true
        },
        {
          "letter": "C",
          "text": "La gestion des clusters",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "Le monitoring des applications",
          "is_correct": false
        }
      ],
      "correct_answer": "B",
      "conseil": "Les R Connectors permettent aux data scientists d'utiliser R pour analyser des donn√©es Hadoop sans apprendre Java ou Pig Latin."
    },
    {
      "question_number": 23,
      "question_text": "Quel est le r√¥le principal de YARN dans Hadoop ?",
      "options": [
        {
          "letter": "A",
          "text": "Stockage des donn√©es",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Gestion des ressources du cluster",
          "is_correct": true
        },
        {
          "letter": "C",
          "text": "Traitement des requ√™tes SQL",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "Collecte des logs",
          "is_correct": false
        }
      ],
      "correct_answer": "B",
      "conseil": "YARN = Resource Manager global du cluster. Il alloue CPU et m√©moire aux applications (MapReduce ou autres) de mani√®re dynamique."
    },
    {
      "question_number": 24,
      "question_text": "Quel est le r√¥le principal de MapReduce dans Hadoop ?",
      "options": [
        {
          "letter": "A",
          "text": "Stockage distribu√© des donn√©es",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Gestion des ressources",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "Traitement distribu√© des donn√©es",
          "is_correct": true
        },
        {
          "letter": "D",
          "text": "Monitoring du cluster",
          "is_correct": false
        }
      ],
      "correct_answer": "C",
      "conseil": "MapReduce = mod√®le de programmation pour le traitement parall√®le. Map (transformation) + Reduce (agr√©gation) sur les donn√©es distribu√©es."
    },
    {
      "question_number": 25,
      "question_text": "Parmi les outils suivants, lequel fonctionne directement au-dessus de HDFS (pas via YARN/MapReduce) ?",
      "options": [
        {
          "letter": "A",
          "text": "Pig",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Hive",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "HBase",
          "is_correct": true
        },
        {
          "letter": "D",
          "text": "Mahout",
          "is_correct": false
        }
      ],
      "correct_answer": "C",
      "conseil": "HBase et Impala fonctionnent directement sur HDFS. Pig, Hive, Mahout g√©n√®rent des jobs MapReduce qui passent par YARN."
    },
    {
      "question_number": 26,
      "question_text": "L'√©cosyst√®me Hadoop permet toutes ces fonctions SAUF :",
      "options": [
        {
          "letter": "A",
          "text": "L'extraction et le stockage des donn√©es",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "La simplification des op√©rations de traitement",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "La gestion et coordination de la plateforme",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "Le remplacement complet des bases de donn√©es relationnelles pour tous les cas d'usage",
          "is_correct": true
        }
      ],
      "correct_answer": "D",
      "conseil": "Hadoop compl√®te mais ne remplace pas totalement les SGBD relationnels. Les SGBD sont toujours meilleurs pour les transactions ACID et les requ√™tes √† faible latence."
    },
    {
      "question_number": 27,
      "question_text": "Quelle caract√©ristique permet √† Hadoop de continuer √† fonctionner m√™me en cas de d√©faillance mat√©rielle ?",
      "options": [
        {
          "letter": "A",
          "text": "Scalabilit√©",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Tol√©rance aux pannes",
          "is_correct": true
        },
        {
          "letter": "C",
          "text": "HPC",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "S√©curit√©",
          "is_correct": false
        }
      ],
      "correct_answer": "B",
      "conseil": "Tol√©rance aux pannes = r√©plication des donn√©es (facteur 3 par d√©faut) + red√©marrage automatique des t√¢ches en cas de panne d'un n≈ìud."
    },
    {
      "question_number": 28,
      "question_text": "HPC dans le contexte Hadoop signifie :",
      "options": [
        {
          "letter": "A",
          "text": "Hadoop Processing Center",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "High Performance Computing",
          "is_correct": true
        },
        {
          "letter": "C",
          "text": "Hadoop Protocol Configuration",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "High Priority Cluster",
          "is_correct": false
        }
      ],
      "correct_answer": "B",
      "conseil": "HPC = High Performance Computing. Hadoop permet le calcul haute performance en parall√©lisant le traitement sur des milliers de n≈ìuds."
    },
    {
      "question_number": 29,
      "question_text": "Quel outil de l'√©cosyst√®me Hadoop est d√©velopp√© par Cloudera pour les requ√™tes SQL ?",
      "options": [
        {
          "letter": "A",
          "text": "Hive",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Phoenix",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "Impala",
          "is_correct": true
        },
        {
          "letter": "D",
          "text": "Pig",
          "is_correct": false
        }
      ],
      "correct_answer": "C",
      "conseil": "Impala = projet Cloudera pour requ√™tes SQL interactives sur HDFS/HBase. Phoenix = projet Apache pour SQL sur HBase."
    },
    {
      "question_number": 30,
      "question_text": "Les donn√©es dans Hadoop sont d'abord divis√©es puis :",
      "options": [
        {
          "letter": "A",
          "text": "Compress√©es et envoy√©es √† un serveur central",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Sauvegard√©es sur un cluster et trait√©es localement",
          "is_correct": true
        },
        {
          "letter": "C",
          "text": "Converties en format propri√©taire",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "Archiv√©es et supprim√©es",
          "is_correct": false
        }
      ],
      "correct_answer": "B",
      "conseil": "Principe Hadoop : Diviser ‚Üí Distribuer ‚Üí Traiter localement ‚Üí Agr√©ger. Pas de centralisation des donn√©es pour le traitement."
    },
    {
      "question_number": 31,
      "question_text": "Quel est l'avantage principal de traiter les donn√©es l√† o√π elles sont stock√©es ?",
      "options": [
        {
          "letter": "A",
          "text": "Meilleure compression",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "R√©duction du co√ªt de transfert de donn√©es",
          "is_correct": true
        },
        {
          "letter": "C",
          "text": "Meilleure s√©curit√©",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "Format de donn√©es uniforme",
          "is_correct": false
        }
      ],
      "correct_answer": "B",
      "conseil": "Transf√©rer 20 To sur le r√©seau = heures/jours. Envoyer du code (quelques Ko) vers les donn√©es = secondes. C'est l'essence de \"data locality\"."
    },
    {
      "question_number": 32,
      "question_text": "Combien de cat√©gories d'outils composent l'√©cosyst√®me Hadoop selon le cours ?",
      "options": [
        {
          "letter": "A",
          "text": "2",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "3",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "4",
          "is_correct": true
        },
        {
          "letter": "D",
          "text": "5",
          "is_correct": false
        }
      ],
      "correct_answer": "C",
      "conseil": "4 cat√©gories : 1) Outils au-dessus de YARN/MR (Pig, Hive...) 2) Outils sur HDFS (HBase, Impala) 3) Connexion externe (Sqoop, Flume) 4) Administration (Ambari, Zookeeper)."
    },
    {
      "question_number": 33,
      "question_text": "Apache Phoenix est un moteur de base de donn√©es relationnelle construit sur :",
      "options": [
        {
          "letter": "A",
          "text": "MongoDB",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Cassandra",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "HBase",
          "is_correct": true
        },
        {
          "letter": "D",
          "text": "HDFS directement",
          "is_correct": false
        }
      ],
      "correct_answer": "C",
      "conseil": "Phoenix ajoute une couche SQL sur HBase, permettant d'utiliser des requ√™tes SQL standards sur une base NoSQL orient√©e colonnes."
    },
    {
      "question_number": 34,
      "question_text": "Quel outil permet de d√©finir des workflows de jobs dans Hadoop ?",
      "options": [
        {
          "letter": "A",
          "text": "Ambari",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Zookeeper",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "Oozie",
          "is_correct": true
        },
        {
          "letter": "D",
          "text": "Flume",
          "is_correct": false
        }
      ],
      "correct_answer": "C",
      "conseil": "Oozie = scheduler de workflows. Il permet de d√©finir des DAG (graphes orient√©s acycliques) de jobs avec d√©pendances et conditions."
    },
    {
      "question_number": 35,
      "question_text": "La scalabilit√© horizontale dans Hadoop signifie :",
      "options": [
        {
          "letter": "A",
          "text": "Ajouter plus de RAM √† un serveur existant",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Ajouter plus de disques √† un serveur existant",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "Ajouter plus de machines au cluster",
          "is_correct": true
        },
        {
          "letter": "D",
          "text": "Augmenter la vitesse du processeur",
          "is_correct": false
        }
      ],
      "correct_answer": "C",
      "conseil": "Horizontale = ajouter des machines (scale out). Verticale = am√©liorer une machine existante (scale up). Hadoop privil√©gie l'horizontal."
    },
    {
      "question_number": 36,
      "question_text": "Quel composant permet la reprise apr√®s √©chec dans Hadoop ?",
      "options": [
        {
          "letter": "A",
          "text": "Les m√©canismes de tol√©rance aux pannes int√©gr√©s",
          "is_correct": true
        },
        {
          "letter": "B",
          "text": "Les sauvegardes manuelles uniquement",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "Le red√©marrage manuel du cluster",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "Un serveur de secours externe",
          "is_correct": false
        }
      ],
      "correct_answer": "A",
      "conseil": "Hadoop g√®re automatiquement les pannes : r√©plication des blocs HDFS, red√©marrage des t√¢ches MapReduce √©chou√©es, basculement NameNode (HA)."
    },
    {
      "question_number": 37,
      "question_text": "Le monitoring du cluster Hadoop peut √™tre fait avec :",
      "options": [
        {
          "letter": "A",
          "text": "Pig",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Hive",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "Ambari",
          "is_correct": true
        },
        {
          "letter": "D",
          "text": "Sqoop",
          "is_correct": false
        }
      ],
      "correct_answer": "C",
      "conseil": "Ambari = interface web pour l'administration et le monitoring. Il affiche les m√©triques, alertes, et permet de g√©rer les services du cluster."
    },
    {
      "question_number": 38,
      "question_text": "Flume est principalement utilis√© pour :",
      "options": [
        {
          "letter": "A",
          "text": "Les requ√™tes SQL",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "La collecte de logs",
          "is_correct": true
        },
        {
          "letter": "C",
          "text": "Le Machine Learning",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "La gestion des workflows",
          "is_correct": false
        }
      ],
      "correct_answer": "B",
      "conseil": "Flume = collecteur de donn√©es en streaming (logs, √©v√©nements) vers HDFS ou HBase. Sources : fichiers logs, syslog, HTTP, etc."
    },
    {
      "question_number": 39,
      "question_text": "Quelle affirmation est correcte concernant Hadoop ?",
      "options": [
        {
          "letter": "A",
          "text": "Il n√©cessite du mat√©riel tr√®s co√ªteux",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Il peut fonctionner sur du mat√©riel standard (commodity hardware)",
          "is_correct": true
        },
        {
          "letter": "C",
          "text": "Il ne supporte que les donn√©es structur√©es",
          "is_correct": false
        },
        {
          "letter": "D",
          "text": "Il est limit√© √† un seul n≈ìud",
          "is_correct": false
        }
      ],
      "correct_answer": "B",
      "conseil": "Hadoop est con√ßu pour fonctionner sur du mat√©riel standard (commodity hardware), pas des serveurs haut de gamme. Cela r√©duit consid√©rablement les co√ªts."
    },
    {
      "question_number": 40,
      "question_text": "Parmi les outils suivants, lequel N'EST PAS un outil de connexion aux sources externes ?",
      "options": [
        {
          "letter": "A",
          "text": "Sqoop",
          "is_correct": false
        },
        {
          "letter": "B",
          "text": "Flume",
          "is_correct": false
        },
        {
          "letter": "C",
          "text": "Mahout",
          "is_correct": true
        },
        {
          "letter": "D",
          "text": "Kafka (dans certaines configurations)",
          "is_correct": false
        }
      ],
      "correct_answer": "C",
      "conseil": "Mahout = biblioth√®que de Machine Learning, pas un connecteur de donn√©es. Sqoop (SGBD), Flume (logs), Kafka (streaming) sont des outils d'ingestion de donn√©es. ## üìä R√©sum√© des Th√®mes Abord√©s | Th√®me | Nombre de Questions | |-------|---------------------| | Architecture Hadoop (HDFS, YARN, MR) | 10 | | √âcosyst√®me et outils | 15 | | Caract√©ristiques (scalabilit√©, tol√©rance...) | 8 | | Concepts fondamentaux | 7 | ## Conseils pour l'Examen 1. M√©morisez les 3 composants de base : HDFS + YARN + MapReduce 2. Connaissez le r√¥le de chaque outil : - Stockage : HDFS, HBase - Traitement : MapReduce, Pig, Hive - Ingestion : Sqoop (SGBD), Flume (logs) - Administration : Ambari, Zookeeper, Oozie 3. Comprenez le principe cl√© : \"D√©placer le calcul vers les donn√©es\" 4. Retenez les caract√©ristiques : Scalabilit√©, Tol√©rance aux pannes, Haute disponibilit√© 5. Distinguez les outils : - Pig = Pig Latin (proc√©dural) - Hive = HiveQL (SQL-like) - Impala = SQL direct sur HDFS (Cloudera) Bonne chance pour votre examen ! üçÄ"
    }
  ]
}